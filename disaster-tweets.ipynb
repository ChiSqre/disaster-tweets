{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Tweets\n",
    "Author: Andrew Farell\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we address the **Natural Language Processing with Disaster Tweets** Kaggle competition. Our objective is to classify Twitter messages as either real disaster-related or not, using a combination of text preprocessing, TF-IDF vectorization, and an XGBoost model for final prediction. The dataset contains tweets with associated features such as *keyword* and *location*, which we leverage to enhance classification performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From an initial review of the training data, we see that it contains 7,613 rows and five columns (id, keyword, location, text, and target), while the test set features 3,263 rows and four columns (id, keyword, location, and text). Notably, the “keyword” and “location” columns contain missing values in both datasets, with “keyword” missing 61 times and “location” missing 2,533 times in the training set, and 26 and 1,105 times, respectively, in the test set. The “id” and “text” fields exhibit no missing entries in either dataset. Among the key statistics for the training set, the mean of the “id” field sits around 5,441, whereas the “target” has a mean of approximately 0.43, indicating that about 43% of the tweets in the training set are disaster-related. Indeed, when examining the target distribution, 4,342 tweets are labeled as non-disaster (0) and 3,271 as disaster (1), suggesting a slightly imbalanced classification. Both datasets’ “id” fields are integers, while “keyword,” “location,” and “text” are strings (object type in Python). The descriptive statistics confirm that “text” length likely varies significantly, pointing to a need for robust text preprocessing. Overall, the data appears suitable for a binary classification task, with moderate imbalance in the target variable and some notable null values in the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "train = pd.read_csv(\"./nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"./nlp-getting-started/test.csv\")\n",
    "sample_sub = pd.read_csv(\"./nlp-getting-started/sample_submission.csv\")\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "print(sample_sub.head())\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(train.columns)\n",
    "print(test.columns)\n",
    "print(train.dtypes)\n",
    "print(test.dtypes)\n",
    "print(train.describe())\n",
    "print(test.describe())\n",
    "print(train.isnull().sum())\n",
    "print(test.isnull().sum())\n",
    "print(train['target'].value_counts())\n",
    "train['keyword'].value_counts().head(20).plot(kind='bar')\n",
    "plt.title(\"Top 20 Keywords in Train\")\n",
    "plt.show()\n",
    "train['location'].value_counts().head(20).plot(kind='bar')\n",
    "plt.title(\"Top 20 Locations in Train\")\n",
    "plt.show()\n",
    "train['target'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Target Distribution in Train\")\n",
    "plt.show()\n",
    "train['text_length'] = train['text'].astype(str).apply(len)\n",
    "train['text_length'].hist(bins=20)\n",
    "plt.title(\"Text Length Distribution\")\n",
    "plt.show()\n",
    "all_text = \" \".join(train['text'].astype(str))\n",
    "wordcloud = WordCloud().generate(all_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ryanfarell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Detailed Hyperparameter Optimization Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <th>param_clf__max_depth</th>\n",
       "      <th>param_clf__learning_rate</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.835560</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>0.481582</td>\n",
       "      <td>0.048366</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.789645</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.429578</td>\n",
       "      <td>0.036956</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.774295</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>0.413946</td>\n",
       "      <td>0.035129</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.777236</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.406790</td>\n",
       "      <td>0.030070</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.726116</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.398205</td>\n",
       "      <td>0.040840</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.720442</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>0.395858</td>\n",
       "      <td>0.045073</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.708609</td>\n",
       "      <td>0.007807</td>\n",
       "      <td>0.379601</td>\n",
       "      <td>0.032958</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.707337</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>0.379312</td>\n",
       "      <td>0.033983</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.635880</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.353595</td>\n",
       "      <td>0.020976</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.638201</td>\n",
       "      <td>0.017171</td>\n",
       "      <td>0.329462</td>\n",
       "      <td>0.025304</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.620383</td>\n",
       "      <td>0.024571</td>\n",
       "      <td>0.315506</td>\n",
       "      <td>0.016469</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.544686</td>\n",
       "      <td>0.035266</td>\n",
       "      <td>0.275478</td>\n",
       "      <td>0.038735</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_clf__n_estimators  param_clf__max_depth  param_clf__learning_rate  \\\n",
       "11                      200                     5                      0.20   \n",
       "9                       200                     3                      0.20   \n",
       "7                       200                     5                      0.10   \n",
       "10                      100                     5                      0.20   \n",
       "8                       100                     3                      0.20   \n",
       "5                       200                     3                      0.10   \n",
       "3                       200                     5                      0.05   \n",
       "6                       100                     5                      0.10   \n",
       "1                       200                     3                      0.05   \n",
       "4                       100                     3                      0.10   \n",
       "2                       100                     5                      0.05   \n",
       "0                       100                     3                      0.05   \n",
       "\n",
       "    mean_train_score  std_train_score  mean_test_score  std_test_score  \\\n",
       "11          0.835560         0.005193         0.481582        0.048366   \n",
       "9           0.789645         0.007820         0.429578        0.036956   \n",
       "7           0.774295         0.008593         0.413946        0.035129   \n",
       "10          0.777236         0.004570         0.406790        0.030070   \n",
       "8           0.726116         0.005662         0.398205        0.040840   \n",
       "5           0.720442         0.004515         0.395858        0.045073   \n",
       "3           0.708609         0.007807         0.379601        0.032958   \n",
       "6           0.707337         0.005993         0.379312        0.033983   \n",
       "1           0.635880         0.015903         0.353595        0.020976   \n",
       "4           0.638201         0.017171         0.329462        0.025304   \n",
       "2           0.620383         0.024571         0.315506        0.016469   \n",
       "0           0.544686         0.035266         0.275478        0.038735   \n",
       "\n",
       "    rank_test_score  \n",
       "11                1  \n",
       "9                 2  \n",
       "7                 3  \n",
       "10                4  \n",
       "8                 5  \n",
       "5                 6  \n",
       "3                 7  \n",
       "6                 8  \n",
       "1                 9  \n",
       "4                10  \n",
       "2                11  \n",
       "0                12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'clf__learning_rate': 0.2, 'clf__max_depth': 5, 'clf__n_estimators': 200}\n",
      "Best CV F1 Score: 0.48158234237753117\n",
      "\n",
      "Refitting best XGB with an eval_set to view training loss progression:\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.67070\n",
      "[1]\tvalidation_0-logloss:0.66040\n",
      "[2]\tvalidation_0-logloss:0.65232\n",
      "[3]\tvalidation_0-logloss:0.64465\n",
      "[4]\tvalidation_0-logloss:0.63790\n",
      "[5]\tvalidation_0-logloss:0.63224\n",
      "[6]\tvalidation_0-logloss:0.62687\n",
      "[7]\tvalidation_0-logloss:0.62196\n",
      "[8]\tvalidation_0-logloss:0.61708\n",
      "[9]\tvalidation_0-logloss:0.61260\n",
      "[10]\tvalidation_0-logloss:0.60824\n",
      "[11]\tvalidation_0-logloss:0.60405\n",
      "[12]\tvalidation_0-logloss:0.60021\n",
      "[13]\tvalidation_0-logloss:0.59658\n",
      "[14]\tvalidation_0-logloss:0.59287\n",
      "[15]\tvalidation_0-logloss:0.58957\n",
      "[16]\tvalidation_0-logloss:0.58653\n",
      "[17]\tvalidation_0-logloss:0.58350\n",
      "[18]\tvalidation_0-logloss:0.58059\n",
      "[19]\tvalidation_0-logloss:0.57728\n",
      "[20]\tvalidation_0-logloss:0.57455\n",
      "[21]\tvalidation_0-logloss:0.57182\n",
      "[22]\tvalidation_0-logloss:0.56935\n",
      "[23]\tvalidation_0-logloss:0.56693\n",
      "[24]\tvalidation_0-logloss:0.56459\n",
      "[25]\tvalidation_0-logloss:0.56215\n",
      "[26]\tvalidation_0-logloss:0.55992\n",
      "[27]\tvalidation_0-logloss:0.55775\n",
      "[28]\tvalidation_0-logloss:0.55517\n",
      "[29]\tvalidation_0-logloss:0.55286\n",
      "[30]\tvalidation_0-logloss:0.55044\n",
      "[31]\tvalidation_0-logloss:0.54799\n",
      "[32]\tvalidation_0-logloss:0.54590\n",
      "[33]\tvalidation_0-logloss:0.54397\n",
      "[34]\tvalidation_0-logloss:0.54172\n",
      "[35]\tvalidation_0-logloss:0.53974\n",
      "[36]\tvalidation_0-logloss:0.53745\n",
      "[37]\tvalidation_0-logloss:0.53565\n",
      "[38]\tvalidation_0-logloss:0.53378\n",
      "[39]\tvalidation_0-logloss:0.53204\n",
      "[40]\tvalidation_0-logloss:0.53040\n",
      "[41]\tvalidation_0-logloss:0.52872\n",
      "[42]\tvalidation_0-logloss:0.52692\n",
      "[43]\tvalidation_0-logloss:0.52536\n",
      "[44]\tvalidation_0-logloss:0.52362\n",
      "[45]\tvalidation_0-logloss:0.52205\n",
      "[46]\tvalidation_0-logloss:0.52043\n",
      "[47]\tvalidation_0-logloss:0.51871\n",
      "[48]\tvalidation_0-logloss:0.51692\n",
      "[49]\tvalidation_0-logloss:0.51551\n",
      "[50]\tvalidation_0-logloss:0.51403\n",
      "[51]\tvalidation_0-logloss:0.51256\n",
      "[52]\tvalidation_0-logloss:0.51118\n",
      "[53]\tvalidation_0-logloss:0.50957\n",
      "[54]\tvalidation_0-logloss:0.50822\n",
      "[55]\tvalidation_0-logloss:0.50666\n",
      "[56]\tvalidation_0-logloss:0.50513\n",
      "[57]\tvalidation_0-logloss:0.50382\n",
      "[58]\tvalidation_0-logloss:0.50252\n",
      "[59]\tvalidation_0-logloss:0.50116\n",
      "[60]\tvalidation_0-logloss:0.49990\n",
      "[61]\tvalidation_0-logloss:0.49851\n",
      "[62]\tvalidation_0-logloss:0.49723\n",
      "[63]\tvalidation_0-logloss:0.49580\n",
      "[64]\tvalidation_0-logloss:0.49422\n",
      "[65]\tvalidation_0-logloss:0.49278\n",
      "[66]\tvalidation_0-logloss:0.49151\n",
      "[67]\tvalidation_0-logloss:0.49032\n",
      "[68]\tvalidation_0-logloss:0.48859\n",
      "[69]\tvalidation_0-logloss:0.48735\n",
      "[70]\tvalidation_0-logloss:0.48610\n",
      "[71]\tvalidation_0-logloss:0.48500\n",
      "[72]\tvalidation_0-logloss:0.48386\n",
      "[73]\tvalidation_0-logloss:0.48264\n",
      "[74]\tvalidation_0-logloss:0.48147\n",
      "[75]\tvalidation_0-logloss:0.48011\n",
      "[76]\tvalidation_0-logloss:0.47875\n",
      "[77]\tvalidation_0-logloss:0.47760\n",
      "[78]\tvalidation_0-logloss:0.47646\n",
      "[79]\tvalidation_0-logloss:0.47548\n",
      "[80]\tvalidation_0-logloss:0.47428\n",
      "[81]\tvalidation_0-logloss:0.47330\n",
      "[82]\tvalidation_0-logloss:0.47192\n",
      "[83]\tvalidation_0-logloss:0.47089\n",
      "[84]\tvalidation_0-logloss:0.46994\n",
      "[85]\tvalidation_0-logloss:0.46892\n",
      "[86]\tvalidation_0-logloss:0.46793\n",
      "[87]\tvalidation_0-logloss:0.46681\n",
      "[88]\tvalidation_0-logloss:0.46571\n",
      "[89]\tvalidation_0-logloss:0.46474\n",
      "[90]\tvalidation_0-logloss:0.46368\n",
      "[91]\tvalidation_0-logloss:0.46270\n",
      "[92]\tvalidation_0-logloss:0.46164\n",
      "[93]\tvalidation_0-logloss:0.46071\n",
      "[94]\tvalidation_0-logloss:0.45983\n",
      "[95]\tvalidation_0-logloss:0.45882\n",
      "[96]\tvalidation_0-logloss:0.45792\n",
      "[97]\tvalidation_0-logloss:0.45702\n",
      "[98]\tvalidation_0-logloss:0.45616\n",
      "[99]\tvalidation_0-logloss:0.45531\n",
      "[100]\tvalidation_0-logloss:0.45437\n",
      "[101]\tvalidation_0-logloss:0.45330\n",
      "[102]\tvalidation_0-logloss:0.45246\n",
      "[103]\tvalidation_0-logloss:0.45163\n",
      "[104]\tvalidation_0-logloss:0.45072\n",
      "[105]\tvalidation_0-logloss:0.44979\n",
      "[106]\tvalidation_0-logloss:0.44901\n",
      "[107]\tvalidation_0-logloss:0.44790\n",
      "[108]\tvalidation_0-logloss:0.44701\n",
      "[109]\tvalidation_0-logloss:0.44621\n",
      "[110]\tvalidation_0-logloss:0.44544\n",
      "[111]\tvalidation_0-logloss:0.44464\n",
      "[112]\tvalidation_0-logloss:0.44389\n",
      "[113]\tvalidation_0-logloss:0.44300\n",
      "[114]\tvalidation_0-logloss:0.44227\n",
      "[115]\tvalidation_0-logloss:0.44152\n",
      "[116]\tvalidation_0-logloss:0.44067\n",
      "[117]\tvalidation_0-logloss:0.43986\n",
      "[118]\tvalidation_0-logloss:0.43913\n",
      "[119]\tvalidation_0-logloss:0.43834\n",
      "[120]\tvalidation_0-logloss:0.43757\n",
      "[121]\tvalidation_0-logloss:0.43687\n",
      "[122]\tvalidation_0-logloss:0.43611\n",
      "[123]\tvalidation_0-logloss:0.43534\n",
      "[124]\tvalidation_0-logloss:0.43456\n",
      "[125]\tvalidation_0-logloss:0.43376\n",
      "[126]\tvalidation_0-logloss:0.43289\n",
      "[127]\tvalidation_0-logloss:0.43219\n",
      "[128]\tvalidation_0-logloss:0.43147\n",
      "[129]\tvalidation_0-logloss:0.43081\n",
      "[130]\tvalidation_0-logloss:0.43012\n",
      "[131]\tvalidation_0-logloss:0.42943\n",
      "[132]\tvalidation_0-logloss:0.42871\n",
      "[133]\tvalidation_0-logloss:0.42787\n",
      "[134]\tvalidation_0-logloss:0.42715\n",
      "[135]\tvalidation_0-logloss:0.42647\n",
      "[136]\tvalidation_0-logloss:0.42563\n",
      "[137]\tvalidation_0-logloss:0.42482\n",
      "[138]\tvalidation_0-logloss:0.42413\n",
      "[139]\tvalidation_0-logloss:0.42335\n",
      "[140]\tvalidation_0-logloss:0.42263\n",
      "[141]\tvalidation_0-logloss:0.42198\n",
      "[142]\tvalidation_0-logloss:0.42127\n",
      "[143]\tvalidation_0-logloss:0.42039\n",
      "[144]\tvalidation_0-logloss:0.41965\n",
      "[145]\tvalidation_0-logloss:0.41882\n",
      "[146]\tvalidation_0-logloss:0.41815\n",
      "[147]\tvalidation_0-logloss:0.41744\n",
      "[148]\tvalidation_0-logloss:0.41683\n",
      "[149]\tvalidation_0-logloss:0.41613\n",
      "[150]\tvalidation_0-logloss:0.41548\n",
      "[151]\tvalidation_0-logloss:0.41482\n",
      "[152]\tvalidation_0-logloss:0.41421\n",
      "[153]\tvalidation_0-logloss:0.41354\n",
      "[154]\tvalidation_0-logloss:0.41280\n",
      "[155]\tvalidation_0-logloss:0.41210\n",
      "[156]\tvalidation_0-logloss:0.41152\n",
      "[157]\tvalidation_0-logloss:0.41089\n",
      "[158]\tvalidation_0-logloss:0.41035\n",
      "[159]\tvalidation_0-logloss:0.40958\n",
      "[160]\tvalidation_0-logloss:0.40900\n",
      "[161]\tvalidation_0-logloss:0.40847\n",
      "[162]\tvalidation_0-logloss:0.40789\n",
      "[163]\tvalidation_0-logloss:0.40735\n",
      "[164]\tvalidation_0-logloss:0.40668\n",
      "[165]\tvalidation_0-logloss:0.40593\n",
      "[166]\tvalidation_0-logloss:0.40535\n",
      "[167]\tvalidation_0-logloss:0.40465\n",
      "[168]\tvalidation_0-logloss:0.40413\n",
      "[169]\tvalidation_0-logloss:0.40327\n",
      "[170]\tvalidation_0-logloss:0.40267\n",
      "[171]\tvalidation_0-logloss:0.40212\n",
      "[172]\tvalidation_0-logloss:0.40154\n",
      "[173]\tvalidation_0-logloss:0.40085\n",
      "[174]\tvalidation_0-logloss:0.40027\n",
      "[175]\tvalidation_0-logloss:0.39976\n",
      "[176]\tvalidation_0-logloss:0.39923\n",
      "[177]\tvalidation_0-logloss:0.39866\n",
      "[178]\tvalidation_0-logloss:0.39814\n",
      "[179]\tvalidation_0-logloss:0.39754\n",
      "[180]\tvalidation_0-logloss:0.39699\n",
      "[181]\tvalidation_0-logloss:0.39636\n",
      "[182]\tvalidation_0-logloss:0.39577\n",
      "[183]\tvalidation_0-logloss:0.39527\n",
      "[184]\tvalidation_0-logloss:0.39478\n",
      "[185]\tvalidation_0-logloss:0.39419\n",
      "[186]\tvalidation_0-logloss:0.39366\n",
      "[187]\tvalidation_0-logloss:0.39306\n",
      "[188]\tvalidation_0-logloss:0.39251\n",
      "[189]\tvalidation_0-logloss:0.39202\n",
      "[190]\tvalidation_0-logloss:0.39156\n",
      "[191]\tvalidation_0-logloss:0.39091\n",
      "[192]\tvalidation_0-logloss:0.39031\n",
      "[193]\tvalidation_0-logloss:0.38983\n",
      "[194]\tvalidation_0-logloss:0.38937\n",
      "[195]\tvalidation_0-logloss:0.38889\n",
      "[196]\tvalidation_0-logloss:0.38832\n",
      "[197]\tvalidation_0-logloss:0.38772\n",
      "[198]\tvalidation_0-logloss:0.38726\n",
      "[199]\tvalidation_0-logloss:0.38673\n",
      "\n",
      "Submission file created: 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "# If you haven't downloaded the stopwords before, you may need the following line:\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return \" \".join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "# --- Load Data ---\n",
    "train = pd.read_csv(\"nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"nlp-getting-started/test.csv\")\n",
    "train.fillna(\"\", inplace=True)\n",
    "test.fillna(\"\", inplace=True)\n",
    "\n",
    "# --- Combine the keyword, location, and text columns ---\n",
    "train[\"full_text\"] = (train[\"keyword\"].astype(str) + \" \" +\n",
    "                      train[\"location\"].astype(str) + \" \" +\n",
    "                      train[\"text\"].astype(str))\n",
    "test[\"full_text\"] = (test[\"keyword\"].astype(str) + \" \" +\n",
    "                     test[\"location\"].astype(str) + \" \" +\n",
    "                     test[\"text\"].astype(str))\n",
    "\n",
    "# --- Preprocess text (lowercase, remove punctuation, remove stopwords) ---\n",
    "train[\"full_text\"] = train[\"full_text\"].apply(text_preprocess)\n",
    "test[\"full_text\"] = test[\"full_text\"].apply(text_preprocess)\n",
    "\n",
    "X = train[\"full_text\"]\n",
    "y = train[\"target\"]\n",
    "\n",
    "# --- Create a pipeline with TF-IDF and an XGBoost classifier ---\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), min_df=3)),\n",
    "    (\"clf\", XGBClassifier(eval_metric=\"logloss\"))\n",
    "])\n",
    "\n",
    "# --- Hyperparameter grid ---\n",
    "params = {\n",
    "    \"clf__n_estimators\": [100, 200],\n",
    "    \"clf__max_depth\": [3, 5],\n",
    "    \"clf__learning_rate\": [0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# --- Grid Search with CV; return_train_score=True to see training performance ---\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    params,\n",
    "    scoring=\"f1\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "grid.fit(X, y)\n",
    "\n",
    "# --- Create a summary table of the cross-validation performance ---\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "summary_cols = [\n",
    "    \"param_clf__n_estimators\",\n",
    "    \"param_clf__max_depth\",\n",
    "    \"param_clf__learning_rate\",\n",
    "    \"mean_train_score\",\n",
    "    \"std_train_score\",\n",
    "    \"mean_test_score\",\n",
    "    \"std_test_score\",\n",
    "    \"rank_test_score\"\n",
    "]\n",
    "cv_summary = cv_results[summary_cols].sort_values(\"rank_test_score\")\n",
    "print(\"Detailed Hyperparameter Optimization Results:\")\n",
    "display(cv_summary)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid.best_params_)\n",
    "print(\"Best CV F1 Score:\", grid.best_score_)\n",
    "\n",
    "# --- After tuning, retrieve the best pipeline and make final predictions ---\n",
    "best_pipeline = grid.best_estimator_\n",
    "preds = best_pipeline.predict(test[\"full_text\"])\n",
    "\n",
    "# --- Optional: Print training loss by refitting best XGB with an eval_set ---\n",
    "#     to track training metrics across iterations\n",
    "print(\"\\nRefitting best XGB with an eval_set to view training loss progression:\\n\")\n",
    "# Extract XGB parameters from best_params_ for clarity\n",
    "best_params = grid.best_params_\n",
    "xgb_params = {\n",
    "    \"n_estimators\": best_params[\"clf__n_estimators\"],\n",
    "    \"max_depth\": best_params[\"clf__max_depth\"],\n",
    "    \"learning_rate\": best_params[\"clf__learning_rate\"],\n",
    "    \"eval_metric\": \"logloss\"\n",
    "}\n",
    "# Transform the entire training set with the fitted TfidfVectorizer\n",
    "tfidf = best_pipeline.named_steps[\"tfidf\"]\n",
    "X_tfidf = tfidf.transform(X)\n",
    "\n",
    "# Create new XGB with best params and track training loss\n",
    "final_xgb = XGBClassifier(**xgb_params)\n",
    "final_xgb.fit(\n",
    "    X_tfidf, y,\n",
    "    eval_set=[(X_tfidf, y)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- Generate submission file ---\n",
    "submission = pd.DataFrame({\"id\": test[\"id\"], \"target\": preds})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nSubmission file created: 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran a grid search on n_estimators, max_depth, and learning_rate using TF-IDF features from the “keyword,” “location,” and “text” columns. The best parameters (n_estimators=200, max_depth=5, learning_rate=0.2) produced a cross-validation F1 of approximately 0.48. After refitting on the full training set, the log loss steadily declined from 0.6707 down to about 0.3867 by the 200th iteration. With these final settings, we generated the “submission.csv” file for Kaggle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
